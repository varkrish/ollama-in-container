version: '3.8'

services:
  ollama-server:
    image: quay.io/varkrish/ollama:latest
    build: 
      context : ./ollama/
      dockerfile: Containerfile
      args:
        - PULL_MODEL_BY_DEFAULT= true
        - MODEL=llama3 #Model should be a valid model
    environment:
      - MODEL=llama3 # Set this if we want to download the models during runtime. Change to mistral or any other model. 
    ports:
      - 11434:11434
    volumes:
     #to cache the models to that we don't need to reload 
      - ./models:/app/ollama/.ollama/
    # Add any additional configuration for your ollama-server service here
    # For example, environment variables, etc.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # change this to appropriatedrive 
              count: 1
              capabilities: [gpu]
    #volumes:
    #Mount any host volume as required.
    # - ./data:/data
  openwebgui:
    image: quay.io/varkrish/openwebgui:latest
    # build: 
    #   context : ./ollama/
    #   dockerfile: Containerfile
    #   args:
    #     - PULL_MODEL_BY_DEFAULT= true
    #     - MODEL=llama3 #Model should be a valid model
    environment:
      -  OLLAMA_API_BASE_URL=http://ollama:11434/api
    ports:
      - 8080:8080


